{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1d3f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tsmoothie.smoother import LowessSmoother\n",
    "from scipy.signal import correlate\n",
    "from scipy.stats import shapiro\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, KBinsDiscretizer\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "import catboost as cb\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c29f6d6",
   "metadata": {},
   "source": [
    "## Reading in data and preparing for merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "668d7a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cases data\n",
    "cases = pd.read_csv(\"./cases_sold.csv\")\n",
    "\n",
    "# Inventory data\n",
    "inv = pd.read_csv(\"./inventory.csv\")\n",
    "inv['BRNCH_CD'] = inv['BRNCH_CD'].astype('string')\n",
    "inv['TEMP_ZONE'] = inv['TEMP_ZONE'].astype('string')\n",
    "inv['TEMP_ZONE'] = inv['TEMP_ZONE'].replace(['CLR', 'DRY', 'FRZ'], ['Refrigerated', 'Dry', 'Freezer'])\n",
    "inv = inv.groupby(['FISC_YR_WK', 'BRNCH_CD']).agg(sum).reset_index()\n",
    "\n",
    "# Spoilage data\n",
    "spoilage = pd.read_csv(\"./spoilage.csv\")\n",
    "spoilage['TEMP_ZONE'] = spoilage['TEMP_ZONE'].replace(['CLR', 'DRY', 'FRZ'], ['Refrigerated', 'Dry', 'Freezer'])\n",
    "spoilage = spoilage.groupby(['FISC_YR_WK', 'BRNCH_CD']).agg(sum).reset_index()\n",
    "\n",
    "# Slot utilization data\n",
    "slot_util = pd.read_csv(\"./Slot Utilization.csv\")\n",
    "slot_util['DATE_EXTRACT'] = pd.to_datetime(slot_util['DATE_EXTRACT'])\n",
    "\n",
    "# Remove rows with no capacity and where branches are equal to X1, X6, or X7 and not stock yards\n",
    "slot_util = slot_util[slot_util['CAPACITY'].notna()]\n",
    "slot_util = slot_util[~slot_util['BRNCH_CD'].isin(['X1', 'X6', 'X7'])]\n",
    "slot_util = slot_util[~slot_util['FULL_MARKET_NAME'].str.contains('STOCK YARDS')]\n",
    "slot_util_main = slot_util[slot_util['CAPACITY'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65f8e89",
   "metadata": {},
   "source": [
    "## Removing manually identified outlier time periods (e.g. extreme capacity values in first few weeks of warehouse opening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17bd7401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note time periods and branchs below were manually selected by looking at capacity over time plots on the EDA dashboard\n",
    "branchs = ['9A', '9J', '8A', '5T', '8T', '9Q', '8B', '4P']\n",
    "period_start = ['2021-04-08', '2021-03-19', '2021-11-30', '2021-06-30', '2021-08-11', '2022-02-08', '2021-10-12', '2022-03-31']\n",
    "period_end = ['2021-04-23', '2021-03-31', '2021-12-04', '2021-08-05', '2021-08-20', '2022-02-09', '2021-10-22', '2022-06-19']\n",
    "\n",
    "# Removing above time periods for selected branches\n",
    "for i, brnch in enumerate(branchs):\n",
    "    slot_util = slot_util[~((slot_util['BRNCH_CD'] == brnch) & (slot_util['DATE_EXTRACT'] >= period_start[i]) & \n",
    "                            (slot_util['DATE_EXTRACT'] <= period_end[i]))]\n",
    "\n",
    "# Removing week of 2021-11-28 (very odd outlier)\n",
    "slot_util = slot_util[~(slot_util['DATE_EXTRACT'] == '2021-11-28')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f6a60",
   "metadata": {},
   "source": [
    "## Derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "665976fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of pallet positions used by day and branch \n",
    "brnchs = slot_util.groupby(['DATE_EXTRACT', 'BRNCH_CD']).agg(np.sum).reset_index()\n",
    "brnchs['CAPACITY'] = brnchs['SUM(PALLET_USED)'] / brnchs['SUM(PALLET_POSITIONS)']\n",
    "brnchs = brnchs[['DATE_EXTRACT', 'BRNCH_CD', 'SUM(PALLET_USED)']]\n",
    "brnchs = brnchs.rename(columns={'SUM(PALLET_USED)':'TOTAL_PALLETS_USED'})\n",
    "\n",
    "# Max number and percentage of pallet positions that are virtual slots by week and branch\n",
    "virt = slot_util.groupby(['DATE_EXTRACT', 'STORAGE_TYPE', 'BRNCH_CD']).agg(np.sum).reset_index()\n",
    "virt = virt.merge(brnchs, how='left', on=['DATE_EXTRACT', 'BRNCH_CD'])\n",
    "virt = virt[['DATE_EXTRACT', 'STORAGE_TYPE', 'BRNCH_CD', 'SUM(PALLET_USED)', 'TOTAL_PALLETS_USED']]\n",
    "virt = virt.rename(columns={'SUM(PALLET_USED)':'VIRTUAL_SLOT_POSITIONS'})\n",
    "virt = virt[virt['STORAGE_TYPE'] == 'Virtual Slot']\n",
    "virt['PERC_VIRTUAL_SLOTS'] = virt['VIRTUAL_SLOT_POSITIONS'] / virt['TOTAL_PALLETS_USED']\n",
    "fw_virt = virt['DATE_EXTRACT'].apply(lambda a : int(str(a.year) + str(a.week)))\n",
    "virt['FISC_YR_WK'] = fw_virt\n",
    "virt = virt.groupby(['FISC_YR_WK', 'BRNCH_CD']).agg(max).reset_index()\n",
    "virt = virt[['FISC_YR_WK', 'BRNCH_CD', 'VIRTUAL_SLOT_POSITIONS', 'PERC_VIRTUAL_SLOTS']]\n",
    "virt = virt.rename(columns={'VIRTUAL_SLOT_POSITIONS':'MAX_VIRTUAL_SLOT_POSITIONS', \n",
    "                            'PERC_VIRTUAL_SLOTS':'MAX_PERC_VIRTUAL_SLOTS'})\n",
    "\n",
    "# Max number and percentage of slots that are actively being picked by week and branch\n",
    "pick = slot_util.groupby(['DATE_EXTRACT', 'PICK_TYPE', 'BRNCH_CD']).agg(np.sum).reset_index()\n",
    "pick = pick.merge(brnchs, how='left', on=['DATE_EXTRACT', 'BRNCH_CD'])\n",
    "pick = pick[['DATE_EXTRACT', 'PICK_TYPE', 'BRNCH_CD', 'SUM(PALLET_USED)', 'TOTAL_PALLETS_USED']]\n",
    "pick = pick.rename(columns={'SUM(PALLET_USED)':'PICK_POSITIONS'})\n",
    "pick = pick[pick['PICK_TYPE'] == 'Pick']\n",
    "pick['PERC_PICK'] = pick['PICK_POSITIONS'] / pick['TOTAL_PALLETS_USED']\n",
    "fw_pick = pick['DATE_EXTRACT'].apply(lambda a : int(str(a.year) + str(a.week)))\n",
    "pick['FISC_YR_WK'] = fw_pick\n",
    "pick = pick.groupby(['FISC_YR_WK', 'BRNCH_CD']).agg(max).reset_index()\n",
    "pick = pick[['FISC_YR_WK', 'BRNCH_CD', 'PICK_POSITIONS', 'PERC_PICK']]\n",
    "pick = pick.rename(columns={'PICK_POSITIONS':'MAX_PICK_POSITIONS', \n",
    "                            'PERC_PICK':'MAX_PERC_PICK'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbc3481",
   "metadata": {},
   "source": [
    "## Merging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec1b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding fiscal year and week to slot_util data to allow merging with other tables\n",
    "fw = slot_util['DATE_EXTRACT'].apply(lambda a : int(str(a.year) + str(a.week)))\n",
    "slot_util['FISC_YR_WK'] = fw\n",
    "slot_util = slot_util.groupby(['BRNCH_CD', 'FISC_YR_WK', 'DATE_EXTRACT']).agg(sum).reset_index()\n",
    "slot_util = slot_util.drop('DATE_EXTRACT', axis=1)\n",
    "slot_util = slot_util.groupby(['BRNCH_CD', 'FISC_YR_WK']).agg(np.mean).reset_index()\n",
    "slot_util['CAPACITY'] = slot_util['SUM(PALLET_USED)'] / slot_util['SUM(PALLET_POSITIONS)']\n",
    "\n",
    "# Merging slot utilization by week and branch\n",
    "merged = slot_util.merge(cases, how='inner', on=['BRNCH_CD', 'FISC_YR_WK'], validate=\"m:1\")\n",
    "merged['DIV_NBR'] = merged['DIV_NBR'].fillna(0)\n",
    "\n",
    "# Merge with inventory data on branch, week, and area\n",
    "merged = merged.merge(inv, how='inner', left_on=['BRNCH_CD', 'FISC_YR_WK'], \n",
    "                      right_on=['BRNCH_CD', 'FISC_YR_WK'], validate=\"m:1\")\n",
    "merged.head()\n",
    "\n",
    "# Merge with derived virtual slot features\n",
    "merged = merged.merge(virt, how='inner', on=['BRNCH_CD', 'FISC_YR_WK'])\n",
    "\n",
    "# Merge with derived pick slot features\n",
    "merged = merged.merge(pick, how='inner', on=['BRNCH_CD', 'FISC_YR_WK'])\n",
    "\n",
    "# Dropping redundant or useless columns\n",
    "final = merged.drop(['WAREHOUSE_LOCN', 'DIV_NBR', 'DIV_NM', 'SUM(PALLET_USED)', 'SUM(PALLET_POSITIONS)'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3784dcb",
   "metadata": {},
   "source": [
    "# Two sets of features - with and without derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c32e4067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRNCH_CD</th>\n",
       "      <th>FISC_YR_WK</th>\n",
       "      <th>CASES_SOLD</th>\n",
       "      <th>MAX_WKLY_INVENTORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2G</td>\n",
       "      <td>202110</td>\n",
       "      <td>125766.5582</td>\n",
       "      <td>504549.312504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2G</td>\n",
       "      <td>202111</td>\n",
       "      <td>130699.0329</td>\n",
       "      <td>505326.354171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2G</td>\n",
       "      <td>202112</td>\n",
       "      <td>131386.2356</td>\n",
       "      <td>494993.050005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2G</td>\n",
       "      <td>202113</td>\n",
       "      <td>136079.6219</td>\n",
       "      <td>497737.160838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2G</td>\n",
       "      <td>202114</td>\n",
       "      <td>129391.5811</td>\n",
       "      <td>514711.669166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  BRNCH_CD  FISC_YR_WK   CASES_SOLD  MAX_WKLY_INVENTORY\n",
       "0       2G      202110  125766.5582       504549.312504\n",
       "1       2G      202111  130699.0329       505326.354171\n",
       "2       2G      202112  131386.2356       494993.050005\n",
       "3       2G      202113  136079.6219       497737.160838\n",
       "4       2G      202114  129391.5811       514711.669166"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_with_derived = final.drop(['CAPACITY'], axis=1)\n",
    "x_base = final.drop(['CAPACITY', 'MAX_PICK_POSITIONS', 'MAX_PERC_PICK', \n",
    "                     'MAX_VIRTUAL_SLOT_POSITIONS', 'MAX_PERC_VIRTUAL_SLOTS'], axis=1)\n",
    "y = final['CAPACITY']\n",
    "x_base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd092d41",
   "metadata": {},
   "source": [
    "## Splitting data and defining standard preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44d8b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected features\n",
    "x = x_with_derived\n",
    "\n",
    "# Train/test split based on selected time period\n",
    "train_start = min(x['FISC_YR_WK'])\n",
    "train_end = max(x['FISC_YR_WK']) - 12\n",
    "indices = (x['FISC_YR_WK'] >= train_start) & (x['FISC_YR_WK'] < train_end)\n",
    "x = x.drop(['FISC_YR_WK'], axis=1)\n",
    "x_train = x[indices]\n",
    "x_test = x[~indices]\n",
    "y_train = y[indices]\n",
    "y_test = y[~indices]\n",
    "x2 = x.drop(['BRNCH_CD'], axis=1)\n",
    "x_train2 = x2[indices]\n",
    "x_test2 = x2[~indices]\n",
    "\n",
    "# Random train/test split\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=12345)\n",
    "\n",
    "# Scoring method\n",
    "scorer = make_scorer(mean_squared_error)\n",
    "\n",
    "# Seed for cross validation\n",
    "cv_seed = 918231\n",
    "\n",
    "process_num_var = Pipeline(steps=[\n",
    "    ('scaling', RobustScaler(with_centering=False))\n",
    "    #('binning', KBinsDiscretizer(n_bins=10))\n",
    "])\n",
    "\n",
    "process_cat_var = Pipeline(steps=[\n",
    "    ('dummy variables', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', process_num_var, x_train.columns[~x_train.columns.isin(['BRNCH_CD', 'AREA'])]),\n",
    "        ('cat', process_cat_var, ['BRNCH_CD'])\n",
    "    ])\n",
    "\n",
    "preprocessor2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', process_num_var, ['CASES_SOLD', 'MAX_WKLY_INVENTORY'])\n",
    "    ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f62ba02",
   "metadata": {},
   "source": [
    "## Comparing different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c0f9bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS RMSE: 0.009098828786171916\n",
      "TEST RMSE: 0.022340573428910965\n"
     ]
    }
   ],
   "source": [
    "# Baseline linear regression model\n",
    "lm = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('lm', LinearRegression())])\n",
    "lm.fit(x_train, y_train)\n",
    "res = cross_validate(lm, x_test, y_test, cv=KFold(n_splits=5, shuffle=True, random_state=cv_seed),\n",
    "                                                    scoring=scorer)\n",
    "print('OLS RMSE:', np.sqrt(np.mean(res['test_score'])))\n",
    "print('TEST RMSE:', np.sqrt(mean_squared_error(y_test, lm.predict(x_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acbc3045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso RMSE: 0.08535642951415125\n",
      "TEST RMSE: 0.08548231956840625\n"
     ]
    }
   ],
   "source": [
    "# Baseline linear regression model\n",
    "lasso = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('lm', Lasso())])\n",
    "lasso.fit(x_train, y_train)\n",
    "res = cross_validate(lasso, x_test, y_test, cv=KFold(n_splits=5, shuffle=True, random_state=cv_seed),\n",
    "                                                    scoring=scorer)\n",
    "print('Lasso RMSE:', np.sqrt(np.mean(res['test_score'])))\n",
    "print('TEST RMSE:', np.sqrt(mean_squared_error(y_test, lasso.predict(x_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3edc267a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART RMSE: 0.019155967183681885\n",
      "TEST RMSE: 0.03061297877708348\n"
     ]
    }
   ],
   "source": [
    "# Baseline decision tree model\n",
    "dt = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('cart', DecisionTreeRegressor())])\n",
    "dt.fit(x_train, y_train)\n",
    "res = cross_validate(dt, x_test, y_test, cv=KFold(n_splits=5, shuffle=True, random_state=cv_seed),\n",
    "                                                    scoring=scorer)\n",
    "print('CART RMSE:', np.sqrt(np.mean(res['test_score'])))\n",
    "print('TEST RMSE:', np.sqrt(mean_squared_error(y_test, dt.predict(x_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc998974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF RMSE: 0.01651764152330299\n",
      "TEST RMSE: 0.021527322493851236\n"
     ]
    }
   ],
   "source": [
    "# Random forest model\n",
    "rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('rf', RandomForestRegressor(n_estimators=500))])\n",
    "rf.fit(x_train, y_train)\n",
    "res = cross_validate(rf, x_test, y_test, cv=KFold(n_splits=5, shuffle=True, random_state=cv_seed),\n",
    "                                                    scoring=scorer)\n",
    "print('RF RMSE:', np.sqrt(np.mean(res['test_score'])))\n",
    "print('TEST RMSE:', np.sqrt(mean_squared_error(y_test, rf.predict(x_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "646abc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boosted__max_depth': 3, 'boosted__n_estimators': 500}\n",
      "sklearn Boosted Trees RMSE: 0.01726871061005853\n",
      "TEST RMSE: 0.02430793024225608\n"
     ]
    }
   ],
   "source": [
    "# Boosted trees model\n",
    "parameters = {'boosted__n_estimators':[500, 1000, 2000, 3000], \n",
    "              'boosted__max_depth':[3, 5, 7]}\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('boosted', GradientBoostingRegressor())])\n",
    "gs = GridSearchCV(pipe, parameters, n_jobs=4)\n",
    "gs.fit(x_train, y_train)\n",
    "print(gs.best_params_)\n",
    "boost = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('boosted', GradientBoostingRegressor(n_estimators=gs.best_params_['boosted__n_estimators'],\n",
    "                                                            max_depth=gs.best_params_['boosted__max_depth']))])\n",
    "boost.fit(x_train, y_train)\n",
    "res = cross_validate(boost, x_test, y_test, cv=KFold(n_splits=5, shuffle=True, random_state=cv_seed),\n",
    "                                                    scoring=scorer)\n",
    "print('sklearn Boosted Trees RMSE:', np.sqrt(np.mean(res['test_score'])))\n",
    "print('TEST RMSE:', np.sqrt(mean_squared_error(y_test, boost.predict(x_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2103e61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boosted__max_depth': 3, 'boosted__n_estimators': 500}\n",
      "sklearn Boosted Trees RMSE: 0.0738934406781873\n"
     ]
    }
   ],
   "source": [
    "# Boosted trees model (without branch)\n",
    "parameters = {'boosted__n_estimators':[500, 1000, 2000, 3000], \n",
    "              'boosted__max_depth':[3, 5, 7]}\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor2),\n",
    "                      ('boosted', GradientBoostingRegressor())])\n",
    "gs = GridSearchCV(pipe, parameters, n_jobs=4)\n",
    "gs.fit(x_train2, y_train)\n",
    "print(gs.best_params_)\n",
    "boost = Pipeline(steps=[('preprocessor', preprocessor2),\n",
    "                      ('boosted', GradientBoostingRegressor(n_estimators=gs.best_params_['boosted__n_estimators'],\n",
    "                                                            max_depth=gs.best_params_['boosted__max_depth']))])\n",
    "boost.fit(x_train2, y_train)\n",
    "res = cross_validate(boost, x_test, y_test, cv=KFold(n_splits=5, shuffle=True, random_state=cv_seed),\n",
    "                                                    scoring=scorer)\n",
    "print('sklearn Boosted Trees RMSE:', np.sqrt(np.mean(res['test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a08a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost RMSE: 0.025492819459601766\n"
     ]
    }
   ],
   "source": [
    "# XGBoost model\n",
    "parameters = {'xgboost__n_estimators':[500, 1000, 2000, 3000], \n",
    "              'xgboost__learning_rate':[0.01, 0.001, 0.0001],\n",
    "              'xgboost__max_depth':[10, 20, 25]}\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('xgboost', xgb.XGBRegressor())])\n",
    "gs = GridSearchCV(pipe, parameters, n_jobs=4)\n",
    "gs.fit(x_train, y_train)\n",
    "xgb_fit = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('xgboost', xgb.XGBRegressor(n_estimators=gs.best_params_['xgboost__n_estimators'],\n",
    "                                                      learning_rate=gs.best_params_['xgboost__learning_rate'],\n",
    "                                                      max_depth=gs.best_params_['xgboost__max_depth']))])\n",
    "xgb_fit.fit(x_train, y_train)\n",
    "res = cross_validate(xgb_fit, x_test, y_test, cv=KFold(n_splits=5, shuffle=True, random_state=cv_seed),\n",
    "                                                    scoring=scorer)\n",
    "print('XGBoost RMSE:', np.sqrt(np.mean(res['test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8f395fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catboost RMSE: 0.01655283219059526\n",
      "TEST RMSE: 0.021673388319102125\n"
     ]
    }
   ],
   "source": [
    "# Catboost model\n",
    "# parameters = {'catboost__iterations':[100, 500, 1000, 2000, None], \n",
    "#               'catboost__learning_rate':[0.01, 0.001, 0.0001, None],\n",
    "#               'catboost__depth':[5, 10, 20, None]}\n",
    "# pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                       ('catboost', cb.CatBoostRegressor(verbose=False))])\n",
    "# gs = GridSearchCV(pipe, parameters, n_jobs=4)\n",
    "# gs.fit(x_train, y_train)\n",
    "# cat_fit = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                           ('catboost', cb.CatBoostRegressor(iterations=gs.best_params_['catboost__iterations'],\n",
    "#                                                             learning_rate=gs.best_params_['catboost__learning_rate'],\n",
    "#                                                             depth=gs.best_params_['catboost__depth']))])\n",
    "cat_fit = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('catboost', cb.CatBoostRegressor(verbose=0, iterations=2000))])\n",
    "cat_fit.fit(x_train, y_train)\n",
    "res = cross_validate(cat_fit, x_test, y_test, cv=KFold(n_splits=5, shuffle=True, random_state=cv_seed),\n",
    "                                                    scoring=scorer)\n",
    "print('Catboost RMSE:', np.sqrt(np.mean(res['test_score'])))\n",
    "print('TEST RMSE:', np.sqrt(mean_squared_error(y_test, cat_fit.predict(x_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a2228c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence intervals (baysian confidence interval package somewhere)\n",
    "# Prediction for just last 1-2 months\n",
    "# One year forecast for capacity based on forecasted cases sold and inventory\n",
    "# Best model with and without warehouse as dummy variable\n",
    "# Include COVID indicator\n",
    "# Try modeling based on different time spans\n",
    "# Pickle best models (whole pipeline ideally)\n",
    "# Add everything to github\n",
    "# Time-series method for automating outlier detection and removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e42984ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('scaling',\n",
       "                                                                   RobustScaler(with_centering=False))]),\n",
       "                                                  ['CASES_SOLD',\n",
       "                                                   'MAX_WKLY_INVENTORY'])])),\n",
       "                ('boosted', GradientBoostingRegressor(n_estimators=500))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selected model to use for forecasting\n",
    "forecast_model = rf\n",
    "forecast_model_no_branch = boost\n",
    "\n",
    "# Refitting models on entire dataset to use for forecasting\n",
    "forecast_model.fit(x, y)\n",
    "forecast_model_no_branch.fit(x2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7fc3a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FISC_YR_WK</th>\n",
       "      <th>CASES_SOLD</th>\n",
       "      <th>BRNCH_CD</th>\n",
       "      <th>MAX_WKLY_INVENTORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202350</td>\n",
       "      <td>329921.82</td>\n",
       "      <td>4O</td>\n",
       "      <td>1.038436e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202316</td>\n",
       "      <td>305104.00</td>\n",
       "      <td>4O</td>\n",
       "      <td>1.038436e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202340</td>\n",
       "      <td>344966.22</td>\n",
       "      <td>4O</td>\n",
       "      <td>1.038436e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>202352</td>\n",
       "      <td>280637.21</td>\n",
       "      <td>4O</td>\n",
       "      <td>1.038436e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202415</td>\n",
       "      <td>316020.37</td>\n",
       "      <td>4O</td>\n",
       "      <td>1.038436e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11555</th>\n",
       "      <td>202523</td>\n",
       "      <td>75922.99</td>\n",
       "      <td>8T</td>\n",
       "      <td>2.868223e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11556</th>\n",
       "      <td>202544</td>\n",
       "      <td>76917.15</td>\n",
       "      <td>8T</td>\n",
       "      <td>2.868223e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11557</th>\n",
       "      <td>202609</td>\n",
       "      <td>76937.44</td>\n",
       "      <td>8T</td>\n",
       "      <td>2.868223e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11558</th>\n",
       "      <td>202620</td>\n",
       "      <td>72209.43</td>\n",
       "      <td>8T</td>\n",
       "      <td>2.868223e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11559</th>\n",
       "      <td>202315</td>\n",
       "      <td>89061.29</td>\n",
       "      <td>8T</td>\n",
       "      <td>2.868223e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11560 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       FISC_YR_WK  CASES_SOLD BRNCH_CD  MAX_WKLY_INVENTORY\n",
       "0          202350   329921.82       4O        1.038436e+06\n",
       "1          202316   305104.00       4O        1.038436e+06\n",
       "2          202340   344966.22       4O        1.038436e+06\n",
       "3          202352   280637.21       4O        1.038436e+06\n",
       "4          202415   316020.37       4O        1.038436e+06\n",
       "...           ...         ...      ...                 ...\n",
       "11555      202523    75922.99       8T        2.868223e+05\n",
       "11556      202544    76917.15       8T        2.868223e+05\n",
       "11557      202609    76937.44       8T        2.868223e+05\n",
       "11558      202620    72209.43       8T        2.868223e+05\n",
       "11559      202315    89061.29       8T        2.868223e+05\n",
       "\n",
       "[11560 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forecast data\n",
    "brnchs = merged.loc[:,['DIV_NBR', 'BRNCH_CD']].drop_duplicates()\n",
    "final_invs = merged.loc[:,['BRNCH_CD', 'FISC_YR_WK']].groupby(['BRNCH_CD']).agg(max).reset_index()\n",
    "final_invs = final_invs.merge(merged.loc[:,['BRNCH_CD', 'FISC_YR_WK', 'MAX_WKLY_INVENTORY']]).drop('FISC_YR_WK', axis=1)\n",
    "forecast = pd.read_csv('./current forecast.csv')\n",
    "forecast = forecast.loc[:,['FISC_YR_WK', 'FORECAST', 'DIV_NBR']]\n",
    "forecast = forecast.merge(brnchs, how='left')\n",
    "forecast = forecast.merge(final_invs)\n",
    "forecast = forecast.drop('DIV_NBR', axis=1)\n",
    "forecast = forecast.rename({\"FORECAST\":\"CASES_SOLD\"}, axis=1)\n",
    "forecast = forecast.dropna()\n",
    "forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a51a2e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASES_SOLD</th>\n",
       "      <th>BRNCH_CD</th>\n",
       "      <th>MAX_WKLY_INVENTORY</th>\n",
       "      <th>PRED_CAPACITY</th>\n",
       "      <th>PRED_CAPACITY_NB</th>\n",
       "      <th>DATE_EXTRACT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>329921.82</td>\n",
       "      <td>4O</td>\n",
       "      <td>1.038436e+06</td>\n",
       "      <td>0.821781</td>\n",
       "      <td>0.844434</td>\n",
       "      <td>2023-12-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>305104.00</td>\n",
       "      <td>4O</td>\n",
       "      <td>1.038436e+06</td>\n",
       "      <td>0.819761</td>\n",
       "      <td>0.789940</td>\n",
       "      <td>2023-04-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>344966.22</td>\n",
       "      <td>4O</td>\n",
       "      <td>1.038436e+06</td>\n",
       "      <td>0.817634</td>\n",
       "      <td>0.850054</td>\n",
       "      <td>2023-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>280637.21</td>\n",
       "      <td>4O</td>\n",
       "      <td>1.038436e+06</td>\n",
       "      <td>0.818637</td>\n",
       "      <td>0.809239</td>\n",
       "      <td>2023-12-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>316020.37</td>\n",
       "      <td>4O</td>\n",
       "      <td>1.038436e+06</td>\n",
       "      <td>0.820981</td>\n",
       "      <td>0.826280</td>\n",
       "      <td>2024-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11555</th>\n",
       "      <td>75922.99</td>\n",
       "      <td>8T</td>\n",
       "      <td>2.868223e+05</td>\n",
       "      <td>0.829683</td>\n",
       "      <td>0.832407</td>\n",
       "      <td>2025-06-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11556</th>\n",
       "      <td>76917.15</td>\n",
       "      <td>8T</td>\n",
       "      <td>2.868223e+05</td>\n",
       "      <td>0.833866</td>\n",
       "      <td>0.832407</td>\n",
       "      <td>2025-10-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11557</th>\n",
       "      <td>76937.44</td>\n",
       "      <td>8T</td>\n",
       "      <td>2.868223e+05</td>\n",
       "      <td>0.834202</td>\n",
       "      <td>0.832407</td>\n",
       "      <td>2026-02-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11558</th>\n",
       "      <td>72209.43</td>\n",
       "      <td>8T</td>\n",
       "      <td>2.868223e+05</td>\n",
       "      <td>0.847627</td>\n",
       "      <td>0.835211</td>\n",
       "      <td>2026-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11559</th>\n",
       "      <td>89061.29</td>\n",
       "      <td>8T</td>\n",
       "      <td>2.868223e+05</td>\n",
       "      <td>0.823661</td>\n",
       "      <td>0.780267</td>\n",
       "      <td>2023-04-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11560 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CASES_SOLD BRNCH_CD  MAX_WKLY_INVENTORY  PRED_CAPACITY  \\\n",
       "0       329921.82       4O        1.038436e+06       0.821781   \n",
       "1       305104.00       4O        1.038436e+06       0.819761   \n",
       "2       344966.22       4O        1.038436e+06       0.817634   \n",
       "3       280637.21       4O        1.038436e+06       0.818637   \n",
       "4       316020.37       4O        1.038436e+06       0.820981   \n",
       "...           ...      ...                 ...            ...   \n",
       "11555    75922.99       8T        2.868223e+05       0.829683   \n",
       "11556    76917.15       8T        2.868223e+05       0.833866   \n",
       "11557    76937.44       8T        2.868223e+05       0.834202   \n",
       "11558    72209.43       8T        2.868223e+05       0.847627   \n",
       "11559    89061.29       8T        2.868223e+05       0.823661   \n",
       "\n",
       "       PRED_CAPACITY_NB DATE_EXTRACT  \n",
       "0              0.844434   2023-12-11  \n",
       "1              0.789940   2023-04-17  \n",
       "2              0.850054   2023-10-02  \n",
       "3              0.809239   2023-12-25  \n",
       "4              0.826280   2024-04-08  \n",
       "...                 ...          ...  \n",
       "11555          0.832407   2025-06-02  \n",
       "11556          0.832407   2025-10-27  \n",
       "11557          0.832407   2026-02-23  \n",
       "11558          0.835211   2026-05-11  \n",
       "11559          0.780267   2023-04-10  \n",
       "\n",
       "[11560 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Capacity forecast made off of forecasted sales data\n",
    "predicted = forecast_model.predict(forecast.drop('FISC_YR_WK', axis=1)[x_train.columns])\n",
    "predicted_no_branch = forecast_model_no_branch.predict(forecast.drop(['FISC_YR_WK', 'BRNCH_CD'], axis=1)[x_train2.columns])\n",
    "forecast['PRED_CAPACITY'] = predicted\n",
    "forecast['PRED_CAPACITY_NB'] = predicted_no_branch\n",
    "forecast['DATE_EXTRACT'] = forecast['FISC_YR_WK'].apply(lambda a : datetime.datetime.strptime(f'{a//100}-W{a%100}-1', '%G-W%V-%u').date())\n",
    "forecast = forecast.drop('FISC_YR_WK', axis=1)\n",
    "forecast.to_csv(\"./forecasted_capacity.csv\", index=False)\n",
    "forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "829757f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions from model using branch as dummy var\n",
      "Mean: 0.7804142229194097\n",
      "Confidence Interval: (0.7789214744476028, 0.7819069713912165)\n",
      "\n",
      "Predictions from model not using branch\n",
      "Mean: 0.787521003565478\n",
      "Confidence Interval: (0.7866692277729138, 0.7883727793580423)\n"
     ]
    }
   ],
   "source": [
    "# Confidence intervals\n",
    "from scipy.stats import bayes_mvs\n",
    "mean_confidence_interval, _, _ = bayes_mvs(predicted, alpha=0.95)\n",
    "print(\"Predictions from model using branch as dummy var\")\n",
    "print(\"Mean:\", mean_confidence_interval[0])\n",
    "print(\"Confidence Interval:\", mean_confidence_interval[1])\n",
    "print()\n",
    "mean_confidence_interval, _, _ = bayes_mvs(predicted_no_branch, alpha=0.95)\n",
    "print(\"Predictions from model not using branch\")\n",
    "print(\"Mean:\", mean_confidence_interval[0])\n",
    "print(\"Confidence Interval:\", mean_confidence_interval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74ba756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling trained models\n",
    "# Model without branch as dummy var\n",
    "with open('pipeline_no_branch.pkl', 'wb') as f:\n",
    "    pickle.dump(forecast_model_no_branch, f)\n",
    "\n",
    "# Model with branch as dummy var\n",
    "with open('pipeline_with_branch.pkl', 'wb') as f:\n",
    "    pickle.dump(forecast_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9fe508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
